{"cells":[{"cell_type":"markdown","id":"2c60a963-341f-458a-9bcd-58671c2d377b","metadata":{},"outputs":[],"source":["# Machine Learning Foundation\n","\n","## Course 3, Part a: Logistic Regression LAB\n"]},{"cell_type":"markdown","id":"f21dc06c-4b15-4e84-b4c8-c3567664cd0c","metadata":{},"outputs":[],"source":["## Introduction\n","\n","We will be using the [Human Activity Recognition with Smartphones](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones) database, which was built from the recordings of study participants who carried a smartphone with an embedded inertial sensor while performing activities of daily living (ADL). The objective is to classify the activities the participants performed into one of the six following categories: walking, walking upstairs, walking downstairs, sitting, standing, and laying.\n","\n","The following information is provided for each record in the dataset: \n","\n","- Triaxial acceleration from the accelerometer (total acceleration) and the estimated body acceleration \n","- Triaxial Angular velocity from the gyroscope \n","- A 561-feature vector with time and frequency domain variables \n","- The activity label \n","\n","More information about the features are available on the website linked above.\n"]},{"cell_type":"code","id":"d6c1ad14-9fcc-4453-97cc-bec1c464ee8c","metadata":{},"outputs":[],"source":["!pip install seaborn  \n!pip install pandas\n!pip install numpy\n!pip install  matplotlib\n!pip install scikit-learn"]},{"cell_type":"code","id":"7500a519-7e45-42f6-8c0f-33c13cac83eb","metadata":{},"outputs":[],"source":["def warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn"]},{"cell_type":"code","id":"6a650785-6dd0-44fe-b737-a000f8b97a3e","metadata":{},"outputs":[],"source":["import seaborn as sns, pandas as pd, numpy as np"]},{"cell_type":"markdown","id":"0d31edc2-cbee-4b1a-9634-75edd142e31e","metadata":{},"outputs":[],"source":["## Question 1\n","\n","Import the data and do the following:\n","\n","* Examine the data types--there are many columns, so it might be wise to use value counts.\n","* Determine if the floating point values need to be scaled.\n","* Determine the breakdown of each activity.\n","* Encode the activity label as an integer.\n"]},{"cell_type":"code","id":"80572ef4-180e-465c-b5a8-9a8d513fe276","metadata":{},"outputs":[],"source":["### BEGIN SOLUTION\ndata = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML241EN-SkillsNetwork/labs/datasets/Human_Activity_Recognition_Using_Smartphones_Data.csv\", sep=',')"]},{"cell_type":"markdown","id":"6143ae98-68e0-447d-bccf-6b8fc469510c","metadata":{},"outputs":[],"source":["The data columns are all floats except for the activity label.\n"]},{"cell_type":"code","id":"3ebd45f3-b9e7-4de4-ab50-33190dc52f19","metadata":{},"outputs":[],"source":["data.dtypes.value_counts()"]},{"cell_type":"code","id":"b622d61d-162f-405a-8442-cd8887322393","metadata":{},"outputs":[],"source":["data.dtypes.tail()"]},{"cell_type":"markdown","id":"a2a02e78-d20c-406e-899d-35c153b5f5dc","metadata":{},"outputs":[],"source":["The data are all scaled from -1 (minimum) to 1.0 (maximum).\n"]},{"cell_type":"code","id":"b2c6a05a-6747-4cf9-8bfc-3c8aba94881a","metadata":{},"outputs":[],"source":["data.iloc[:, :-1].min().value_counts()"]},{"cell_type":"code","id":"ef44fd25-8ea1-4848-9d92-1b3782072ab7","metadata":{},"outputs":[],"source":["data.iloc[:, :-1].max().value_counts()"]},{"cell_type":"markdown","id":"b3c44b8c-70b4-4f2f-be9f-1c5b9b727b05","metadata":{},"outputs":[],"source":["Examine the breakdown of activities; they are relatively balanced.\n"]},{"cell_type":"code","id":"319e4dfb-3284-4cc2-a202-ed63736dfb01","metadata":{},"outputs":[],"source":["data.Activity.value_counts()"]},{"cell_type":"markdown","id":"f991b67d-1668-49f4-ad08-e9231df1f5fd","metadata":{},"outputs":[],"source":["Scikit learn classifiers won't accept a sparse matrix for the prediction column. Thus, either `LabelEncoder` needs to be used to convert the activity labels to integers, or if `DictVectorizer` is used, the resulting matrix must be converted to a non-sparse array.  \n","Use `LabelEncoder` to fit_transform the \"Activity\" column, and look at 5 random values.\n"]},{"cell_type":"code","id":"b141d836-9798-4cb0-9b7c-bc298b094dd4","metadata":{},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ndata['Activity'] = le.fit_transform(data.Activity)\ndata['Activity'].sample(5)\n### END SOLUTION"]},{"cell_type":"markdown","id":"e6ff9222-961e-4570-9a2f-e4970d6ef226","metadata":{},"outputs":[],"source":["## Question 2\n","\n","* Calculate the correlations between the dependent variables.\n","* Create a histogram of the correlation values.\n","* Identify those that are most correlated (either positively or negatively).\n"]},{"cell_type":"code","id":"190d9513-67d6-405f-ad9f-8c075af15dfb","metadata":{},"outputs":[],"source":["### BEGIN SOLUTION\n# Calculate the correlation values\nfeature_cols = data.columns[:-1]\ncorr_values = data[feature_cols].corr()\n\n# Simplify by emptying all the data below the diagonal\ntril_index = np.tril_indices_from(corr_values)\n\n# Make the unused values NaNs\nfor coord in zip(*tril_index):\n    corr_values.iloc[coord[0], coord[1]] = np.NaN\n    \n# Stack the data and convert to a data frame\ncorr_values = (corr_values\n               .stack()\n               .to_frame()\n               .reset_index()\n               .rename(columns={'level_0':'feature1',\n                                'level_1':'feature2',\n                                0:'correlation'}))\n\n# Get the absolute values for sorting\ncorr_values['abs_correlation'] = corr_values.correlation.abs()"]},{"cell_type":"markdown","id":"0bae03dc-63d4-4b12-b1e9-e9a3f54ab44e","metadata":{},"outputs":[],"source":["A histogram of the absolute value correlations.\n"]},{"cell_type":"code","id":"bbc5b722-3020-4452-ada2-57e3981996d3","metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline"]},{"cell_type":"code","id":"066e45f0-d5b4-420b-abcb-811565ee6c36","metadata":{},"outputs":[],"source":["sns.set_context('talk')\nsns.set_style('white')\n\nax = corr_values.abs_correlation.hist(bins=50, figsize=(12, 8))\nax.set(xlabel='Absolute Correlation', ylabel='Frequency');"]},{"cell_type":"code","id":"536f8297-03fa-48b3-a9c0-8d68cfeeccdd","metadata":{},"outputs":[],"source":["# The most highly correlated values\ncorr_values.sort_values('correlation', ascending=False).query('abs_correlation>0.8')\n### END SOLUTION"]},{"cell_type":"markdown","id":"b9081cdc-84a8-453e-9fb5-f96beafa4695","metadata":{},"outputs":[],"source":["## Question 3\n","\n","* Split the data into train and test data sets. This can be done using any method, but consider using Scikit-learn's `StratifiedShuffleSplit` to maintain the same ratio of predictor classes.\n","* Regardless of the method used to split the data, compare the ratio of classes in both the train and test splits.\n"]},{"cell_type":"code","id":"72503e40-f74e-43d0-99f7-da9cc2f3e2c3","metadata":{},"outputs":[],"source":["### BEGIN SOLUTION\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\n# Get the split indexes\nstrat_shuf_split = StratifiedShuffleSplit(n_splits=1, \n                                          test_size=0.3, \n                                          random_state=42)\n\ntrain_idx, test_idx = next(strat_shuf_split.split(data[feature_cols], data.Activity))\n\n# Create the dataframes\nX_train = data.loc[train_idx, feature_cols]\ny_train = data.loc[train_idx, 'Activity']\n\nX_test  = data.loc[test_idx, feature_cols]\ny_test  = data.loc[test_idx, 'Activity']"]},{"cell_type":"code","id":"e4f51d68-3427-4c99-ae2f-02ce679ff8d6","metadata":{},"outputs":[],"source":["y_train.value_counts(normalize=True)"]},{"cell_type":"code","id":"202de9a0-393c-451e-823e-feb46e9b82f1","metadata":{},"outputs":[],"source":["y_test.value_counts(normalize=True)\n### END SOLUTION"]},{"cell_type":"markdown","id":"de52ae3a-7db6-4649-8751-34d547459b07","metadata":{},"outputs":[],"source":["## Question 4\n","\n","* Fit a logistic regression model without any regularization using all of the features. Be sure to read the documentation about fitting a multi-class model so you understand the coefficient output. Store the model.\n","* Using cross validation to determine the hyperparameters and fit models using L1 and L2 regularization. Store each of these models as well. Note the limitations on multi-class models, solvers, and regularizations. The regularized models, in particular the L1 model, will probably take a while to fit.\n"]},{"cell_type":"code","id":"97a66223-4449-4f9c-a1cd-2cb13446ae7c","metadata":{},"outputs":[],"source":["### BEGIN SOLUTION\nfrom sklearn.linear_model import LogisticRegression\n\n# Standard logistic regression\nlr = LogisticRegression(solver='liblinear').fit(X_train, y_train)"]},{"cell_type":"code","id":"18c91c4d-c703-469c-8508-febc2617df12","metadata":{},"outputs":[],"source":["from sklearn.linear_model import LogisticRegressionCV\n\n# L1 regularized logistic regression\nlr_l1 = LogisticRegressionCV(Cs=10, cv=4, penalty='l1', solver='liblinear').fit(X_train, y_train)"]},{"cell_type":"code","id":"976605e2-0ec6-4075-a724-d813d155e035","metadata":{},"outputs":[],"source":["# L2 regularized logistic regression\nlr_l2 = LogisticRegressionCV(Cs=10, cv=4, penalty='l2', solver='liblinear').fit(X_train, y_train)\n### END SOLUTION"]},{"cell_type":"markdown","id":"b385b35d-dc8d-4362-9a88-9a46943b0794","metadata":{},"outputs":[],"source":["## Question 5\n","\n","* Compare the magnitudes of the coefficients for each of the models. If one-vs-rest fitting was used, each set of coefficients can be plotted separately. \n"]},{"cell_type":"code","id":"c3ede455-7a3a-46ed-a356-bf47648e4109","metadata":{},"outputs":[],"source":["### BEGIN SOLUTION\n# Combine all the coefficients into a dataframe\ncoefficients = list()\n\ncoeff_labels = ['lr', 'l1', 'l2']\ncoeff_models = [lr, lr_l1, lr_l2]\n\nfor lab,mod in zip(coeff_labels, coeff_models):\n    coeffs = mod.coef_\n    coeff_label = pd.MultiIndex(levels=[[lab], [0,1,2,3,4,5]], \n                                 codes=[[0,0,0,0,0,0], [0,1,2,3,4,5]])\n    coefficients.append(pd.DataFrame(coeffs.T, columns=coeff_label))\n\ncoefficients = pd.concat(coefficients, axis=1)\n\ncoefficients.sample(10)"]},{"cell_type":"markdown","id":"7c53560e-56ce-49ad-a33e-d29a5a86156f","metadata":{},"outputs":[],"source":["Prepare six separate plots for each of the multi-class coefficients.\n"]},{"cell_type":"code","id":"a77b63a2-d6ac-44b3-8916-f17ea681cb9b","metadata":{},"outputs":[],"source":["fig, axList = plt.subplots(nrows=3, ncols=2)\naxList = axList.flatten()\nfig.set_size_inches(10,10)\n\nfor ax in enumerate(axList):\n    loc = ax[0]\n    ax = ax[1]\n    \n    data = coefficients.xs(loc, level=1, axis=1)\n    data.plot(marker='o', ls='', ms=2.0, ax=ax, legend=False)\n    \n    if ax is axList[0]:\n        ax.legend(loc=4)\n        \n    ax.set(title='Coefficient Set '+str(loc))\n\nplt.tight_layout()\n### END SOLUTION"]},{"cell_type":"markdown","id":"02ae8563-26f1-4a7e-a245-9b66b6867437","metadata":{},"outputs":[],"source":["## Question 6\n","\n","* Predict and store the class for each model.\n","* Store the probability for the predicted class for each model. \n"]},{"cell_type":"code","id":"6432df2d-3dcc-44c2-ad5e-1e4cab14dcf8","metadata":{},"outputs":[],"source":["### BEGIN SOLUTION\n# Predict the class and the probability for each\ny_pred = list()\ny_prob = list()\n\ncoeff_labels = ['lr', 'l1', 'l2']\ncoeff_models = [lr, lr_l1, lr_l2]\n\nfor lab,mod in zip(coeff_labels, coeff_models):\n    y_pred.append(pd.Series(mod.predict(X_test), name=lab))\n    y_prob.append(pd.Series(mod.predict_proba(X_test).max(axis=1), name=lab))\n    \ny_pred = pd.concat(y_pred, axis=1)\ny_prob = pd.concat(y_prob, axis=1)\n\ny_pred.head()"]},{"cell_type":"code","id":"e8f6a455-017f-473e-be73-62b9bc6c6920","metadata":{},"outputs":[],"source":["y_prob.head()\n### END SOLUTION"]},{"cell_type":"markdown","id":"fdadfaa4-fc3f-41b1-bad4-0ce3acccbc11","metadata":{},"outputs":[],"source":["## Question 7\n","\n","For each model, calculate the following error metrics: \n","\n","* Accuracy\n","* Precision\n","* Recall\n","* F-score\n","* Confusion Matrix\n","\n","Decide how to combine the multi-class metrics into a single value for each model.\n"]},{"cell_type":"code","id":"e3d191c7-5de2-4778-af1a-5a30c0f8036a","metadata":{},"outputs":[],"source":["### BEGIN SOLUTION\nfrom sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\nfrom sklearn.preprocessing import label_binarize\n\nmetrics = list()\ncm = dict()\n\nfor lab in coeff_labels:\n\n    # Preciision, recall, f-score from the multi-class support function\n    precision, recall, fscore, _ = score(y_test, y_pred[lab], average='weighted')\n    \n    # The usual way to calculate accuracy\n    accuracy = accuracy_score(y_test, y_pred[lab])\n    \n    # ROC-AUC scores can be calculated by binarizing the data\n    auc = roc_auc_score(label_binarize(y_test, classes=[0,1,2,3,4,5]),\n              label_binarize(y_pred[lab], classes=[0,1,2,3,4,5]), \n              average='weighted')\n    \n    # Last, the confusion matrix\n    cm[lab] = confusion_matrix(y_test, y_pred[lab])\n    \n    metrics.append(pd.Series({'precision':precision, 'recall':recall, \n                              'fscore':fscore, 'accuracy':accuracy,\n                              'auc':auc}, \n                             name=lab))\n\nmetrics = pd.concat(metrics, axis=1)"]},{"cell_type":"code","id":"ba2aea97-72e6-4293-981c-5d833a884c54","metadata":{},"outputs":[],"source":["metrics\n### END SOLUTION"]},{"cell_type":"markdown","id":"61e91083-855d-46af-9b44-1137d3720f61","metadata":{},"outputs":[],"source":["## Question 8\n","\n","* Display or plot the confusion matrix for each model.\n"]},{"cell_type":"code","id":"4d2a6264-0f21-4da2-9520-e913b3101c64","metadata":{},"outputs":[],"source":["### BEGIN SOLUTION\nfig, axList = plt.subplots(nrows=2, ncols=2)\naxList = axList.flatten()\nfig.set_size_inches(12, 10)\n\naxList[-1].axis('off')\n\nfor ax,lab in zip(axList[:-1], coeff_labels):\n    sns.heatmap(cm[lab], ax=ax, annot=True, fmt='d');\n    ax.set(title=lab);\n    \nplt.tight_layout()\n### END SOLUTION"]},{"cell_type":"markdown","id":"6989f4da-f87c-4d3e-bb01-09c5f5e8e6a9","metadata":{},"outputs":[],"source":["---\n","### Machine Learning Foundation (C) 2020 IBM Corporation\n"]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"language_info":{"name":"python","version":"3.11.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"prev_pub_hash":"5595500a1b20d83ab72984280f4996e5d926d4c49321ec6e080dd624afd105cc"},"nbformat":4,"nbformat_minor":4}